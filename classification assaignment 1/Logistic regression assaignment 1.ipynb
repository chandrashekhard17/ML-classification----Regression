{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7a04fab",
   "metadata": {},
   "source": [
    "### Q1. Difference Between Linear Regression and Logistic Regression\n",
    "- **Linear Regression** is used for predicting continuous numeric values. It models the relationship between a dependent variable \\( Y \\) and one or more independent variables \\( X \\) using a linear equation: \\( Y = \\beta_0 + \\beta_1X + \\epsilon \\).\n",
    "- **Logistic Regression** is used for binary classification tasks, predicting the probability of an event belonging to one of two classes. It uses a logistic function to model the probability: \\( P(Y = 1) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1X)}} \\).\n",
    "\n",
    "Example: Logistic regression would be more appropriate for predicting whether a patient has a disease (yes/no) based on their medical data. The output is a probability that is then classified into one of two classes.\n",
    "\n",
    "### Q2. Cost Function in Logistic Regression\n",
    "In logistic regression, the cost function used is the **logarithmic loss**, or **binary cross-entropy**:\n",
    "\\[\n",
    "\\text{Cost}(h_\\theta(x), y) = - \\left[ y \\log(h_\\theta(x)) + (1 - y) \\log(1 - h_\\theta(x)) \\right]\n",
    "\\]\n",
    "This cost function is minimized using **Gradient Descent**, where the parameters are iteratively updated to reduce the cost until convergence.\n",
    "\n",
    "### Q3. Regularization in Logistic Regression\n",
    "**Regularization** helps prevent overfitting by adding a penalty term to the cost function. Common types include:\n",
    "- **L2 regularization** (Ridge): Adds \\( \\lambda \\sum \\theta_i^2 \\), where \\( \\lambda \\) is the regularization parameter.\n",
    "- **L1 regularization** (Lasso): Adds \\( \\lambda \\sum |\\theta_i| \\), which can also lead to feature selection by driving some coefficients to zero.\n",
    "\n",
    "Regularization reduces the model's complexity by penalizing large coefficients, thus helping to generalize the model to new data.\n",
    "\n",
    "### Q4. ROC Curve and Its Use\n",
    "The **Receiver Operating Characteristic (ROC) curve** is a graphical representation of a model's performance. It plots the **True Positive Rate (TPR)** against the **False Positive Rate (FPR)** at various threshold settings. The **Area Under the ROC Curve (AUC)** quantifies the modelâ€™s ability to distinguish between classes:\n",
    "- AUC of 1 indicates perfect performance.\n",
    "- AUC of 0.5 indicates random guessing.\n",
    "\n",
    "In logistic regression, a higher AUC indicates better model performance in separating the classes.\n",
    "\n",
    "### Q5. Feature Selection Techniques in Logistic Regression\n",
    "Some common techniques for feature selection include:\n",
    "- **Recursive Feature Elimination (RFE)**: Iteratively removes features and assesses model performance.\n",
    "- **L1 Regularization**: Automatically selects features by shrinking irrelevant coefficients to zero.\n",
    "- **Statistical Tests** (e.g., Chi-square, ANOVA): Evaluate the importance of features.\n",
    "\n",
    "These techniques help reduce dimensionality, improve model interpretability, and prevent overfitting by excluding irrelevant features.\n",
    "\n",
    "### Q6. Handling Imbalanced Datasets in Logistic Regression\n",
    "To handle class imbalance, you can:\n",
    "- **Resample the Data**: Use techniques like oversampling the minority class or undersampling the majority class.\n",
    "- **Adjust Class Weights**: Assign a higher weight to the minority class in the loss function to penalize misclassifications more.\n",
    "- **Use SMOTE (Synthetic Minority Over-sampling Technique)**: Generate synthetic samples for the minority class.\n",
    "\n",
    "These strategies help the model better learn from the minority class, improving its performance on imbalanced datasets.\n",
    "\n",
    "### Q7. Common Issues in Logistic Regression and Solutions\n",
    "- **Multicollinearity**: When independent variables are highly correlated, it can inflate coefficient estimates. To address this, you can:\n",
    "  - Use **Variance Inflation Factor (VIF)** to identify and remove highly correlated features.\n",
    "  - Apply **Principal Component Analysis (PCA)** to reduce dimensions.\n",
    "- **Outliers**: Outliers can distort the model. You can use **robust scaling techniques** or remove outliers based on domain knowledge.\n",
    "- **Overfitting**: Use **regularization** to penalize large coefficients and reduce complexity.\n",
    "\n",
    "By addressing these issues, you can build a more robust logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71440787",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
