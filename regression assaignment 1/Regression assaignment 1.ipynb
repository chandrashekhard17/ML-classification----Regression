{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5b994f9",
   "metadata": {},
   "source": [
    "### Q1. **Difference Between Simple Linear Regression and Multiple Linear Regression**  \n",
    "- **Simple Linear Regression**: In simple linear regression, there is one **independent variable (X)** and one **dependent variable (Y)**. The relationship is modeled by a straight line.\n",
    "\n",
    "   **Example**: Predicting a person’s weight based on their height.\n",
    "   \\[\n",
    "   \\text{Weight} = \\beta_0 + \\beta_1 \\cdot \\text{Height} + \\epsilon\n",
    "   \\]\n",
    "\n",
    "- **Multiple Linear Regression**: In multiple linear regression, there are two or more **independent variables (X1, X2, ..., Xn)** to predict one **dependent variable (Y)**. \n",
    "\n",
    "   **Example**: Predicting a person’s weight based on their height and age.\n",
    "   \\[\n",
    "   \\text{Weight} = \\beta_0 + \\beta_1 \\cdot \\text{Height} + \\beta_2 \\cdot \\text{Age} + \\epsilon\n",
    "   \\]\n",
    "\n",
    "### Q2. **Assumptions of Linear Regression and How to Check Them**\n",
    "1. **Linearity**: The relationship between the independent and dependent variables should be linear.\n",
    "   - **Check**: Use scatter plots of residuals vs. fitted values.\n",
    "\n",
    "2. **Independence**: Observations should be independent of each other.\n",
    "   - **Check**: Time series data can be checked for autocorrelation using the Durbin-Watson test.\n",
    "\n",
    "3. **Homoscedasticity**: The residuals (errors) should have constant variance.\n",
    "   - **Check**: Plot residuals vs. fitted values. Patterns indicate heteroscedasticity.\n",
    "\n",
    "4. **Normality of Residuals**: The residuals should be normally distributed.\n",
    "   - **Check**: Use a Q-Q plot or the Shapiro-Wilk test.\n",
    "\n",
    "5. **No Multicollinearity**: In multiple linear regression, the independent variables should not be highly correlated.\n",
    "   - **Check**: Use the Variance Inflation Factor (VIF).\n",
    "\n",
    "### Q3. **Interpretation of Slope and Intercept in Linear Regression**\n",
    "- **Intercept (\\(\\beta_0\\))**: The intercept is the predicted value of the dependent variable when all the independent variables are zero.\n",
    "- **Slope (\\(\\beta_1\\))**: The slope represents the change in the dependent variable for a one-unit change in the independent variable.\n",
    "\n",
    "**Example**: Suppose you have the model:\n",
    "\\[\n",
    "\\text{Salary} = 30,000 + 5,000 \\cdot \\text{Years of Experience}\n",
    "\\]\n",
    "Here, the intercept (30,000) is the predicted salary for someone with 0 years of experience. The slope (5,000) means that for every additional year of experience, the salary increases by $5,000.\n",
    "\n",
    "### Q4. **Concept of Gradient Descent**\n",
    "Gradient Descent is an optimization algorithm used to minimize a cost function in machine learning, typically in training models like linear regression or neural networks.\n",
    "\n",
    "- **How it works**: It starts with random initial values for the model parameters (like slopes and intercepts), and iteratively adjusts them in the direction that reduces the error between predicted and actual values. The process continues until the error is minimized.\n",
    "  \n",
    "- **In machine learning**: It is used to find the optimal parameters for models by minimizing the cost function (e.g., mean squared error).\n",
    "\n",
    "### Q5. **Multiple Linear Regression Model**\n",
    "- **Formula**: The multiple linear regression model is:\n",
    "  \\[\n",
    "  Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_n X_n + \\epsilon\n",
    "  \\]\n",
    "  Where \\(Y\\) is the dependent variable, and \\(X_1, X_2, ..., X_n\\) are independent variables.\n",
    "\n",
    "- **Difference from Simple Linear Regression**: The key difference is the number of independent variables. In **simple linear regression**, there's only one independent variable, while in **multiple linear regression**, there are multiple independent variables.\n",
    "\n",
    "### Q6. **Multicollinearity in Multiple Linear Regression**\n",
    "- **Concept**: Multicollinearity occurs when two or more independent variables in a regression model are highly correlated. This makes it difficult to isolate the individual effect of each variable on the dependent variable.\n",
    "\n",
    "- **Detecting Multicollinearity**:\n",
    "  - **Variance Inflation Factor (VIF)**: A VIF > 10 suggests high multicollinearity.\n",
    "  - **Correlation Matrix**: Check pairwise correlations between independent variables.\n",
    "\n",
    "- **Addressing Multicollinearity**:\n",
    "  - Remove highly correlated predictors.\n",
    "  - Use techniques like Principal Component Analysis (PCA) to reduce dimensionality.\n",
    "\n",
    "### Q7. **Polynomial Regression Model**\n",
    "- **Concept**: Polynomial regression is an extension of linear regression that fits a non-linear relationship between the independent and dependent variables. The model includes polynomial terms of the independent variables.\n",
    "\n",
    "- **Formula**: For a second-degree polynomial regression:\n",
    "  \\[\n",
    "  Y = \\beta_0 + \\beta_1 X + \\beta_2 X^2 + \\epsilon\n",
    "  \\]\n",
    "\n",
    "- **Difference from Linear Regression**: In **linear regression**, the relationship between \\(X\\) and \\(Y\\) is assumed to be linear. In **polynomial regression**, \\(X\\) can be raised to higher powers (e.g., \\(X^2, X^3\\)) to model non-linear relationships.\n",
    "\n",
    "### Q8. **Advantages and Disadvantages of Polynomial Regression**\n",
    "- **Advantages**:\n",
    "  - Can model non-linear relationships effectively.\n",
    "  - More flexible than linear regression.\n",
    "\n",
    "- **Disadvantages**:\n",
    "  - High-degree polynomials can lead to overfitting, making the model too sensitive to noise.\n",
    "  - Not very interpretable for higher-degree polynomials.\n",
    "\n",
    "- **When to Prefer Polynomial Regression**:\n",
    "  - When the relationship between the independent and dependent variables is non-linear and cannot be captured by a simple straight line.\n",
    "  - Example: Modeling the trajectory of a projectile, where a quadratic relationship (parabola) is more appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a1da32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
